{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function definitions\n",
    "\n",
    "def tag_json(json_string):\n",
    "    \"\"\"\n",
    "    if row can be decoded, tag as good\n",
    "    otherwise, tag as bad. Also log the error type, exception given, and json string\n",
    "    \"\"\"\n",
    "    try:\n",
    "        json.loads(json_string)\n",
    "        return json.dumps({\"good\": json_string})\n",
    "    except Exception as e:\n",
    "        return json.dumps({\"bad\": \"['%s'',''%s'',''%s]\" % (type(e), e.args[0], json_string)})\n",
    "    \n",
    "def decimal_to_string(json_string):\n",
    "    \"\"\"\n",
    "    convert (decimal) value of shodan key to string\n",
    "    \"\"\"\n",
    "    json_string = json.loads(json_string)\n",
    "    try:\n",
    "        json_string[\"location\"][\"longitude\"] = str(json_string[\"location\"][\"longitude\"])\n",
    "    except:\n",
    "        json_string[\"location\"][\"longitude\"] = json_string[\"location\"][\"longitude\"]\n",
    "    try:\n",
    "        json_string[\"location\"][\"latitude\"] = str(json_string[\"location\"][\"latitude\"])\n",
    "    except:\n",
    "        json_string[\"location\"][\"latitude\"] = json_string[\"location\"][\"latitude\"]\n",
    "    try:\n",
    "        json_string[\"port\"] = str(json_string[\"port\"])\n",
    "    except:\n",
    "        json_string[\"port\"] = json_string[\"port\"]\n",
    "    try:\n",
    "        json_string[\"ip\"] = str(json_string[\"ip\"])\n",
    "    except:\n",
    "        json_string[\"ip\"] = json_string[\"ip\"]\n",
    "    try:\n",
    "        json_string[\"ssl\"][\"cert\"][\"serial\"] = str(json_string[\"ssl\"][\"cert\"][\"serial\"])\n",
    "    except:\n",
    "        pass\n",
    "    json_string = json.dumps(json_string)\n",
    "    return json_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load shodan files from HDFS\n",
    "jsn = sc.textFile(\"/data/staging/shodan/raw_uncompressed\", 10000).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#check output after tagging record as \"good\" or \"bad\"\n",
    "tagged = jsn.map(lambda x: tag_json(x))\n",
    "# print tagged.take(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#split tagged RDD into 2 RDDs: shodan (\"good\" and valid records) and errors (\"bad\" records)\n",
    "errors = tagged.filter(lambda x: json.loads(x).keys()[0] == \"bad\").map(lambda x: json.loads(x)[\"bad\"]).cache()\n",
    "shodan = tagged.filter(lambda x: json.loads(x).keys()[0] == \"good\").map(lambda x: json.loads(x)[\"good\"]).map(lambda x: decimal_to_string(x)).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#convert good RDD to dataframe\n",
    "shodanJSON = sqlContext.jsonRDD(shodan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save as gz.parquet file\n",
    "shodanJSON.write.parquet(\"/user/svillanueva/spark_jsonRDD_pq_shodan_201501_FULL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save bad RDD (errors) to file\n",
    "errors.saveAsTextFile(\"spark_shodan_errors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#check shodan columns\n",
    "print shodanJSON.columns\n",
    "print\n",
    "shodanJSON.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create hive table snappy parquet and insert data from gz.parquet\n",
    "#create external table works but insert into snappy parquet doesn't work yet because of compatibility issues with Spark parquet and Hive parquet\n",
    "with open(\"shodan.hql\", \"wb\") as outfile:\n",
    "    outfile.write(\"use shodan;\\nSET hive.exec.dynamic.partition.mode=nonstrict;\\n\" \\\n",
    "                  \"--increase map and reduce memory to handle java heap size error\\n\" \\\n",
    "                  \"SET mapreduce.map.memory.mb=15000;\\n\" \\\n",
    "                 \"SET mapreduce.reduce.memory.mb=5000;\\n\\n\" \\\n",
    "                 \"--create external staging table\\n\" \\\n",
    "                  \"CREATE EXTERNAL TABLE shodan.pq_shodan_staging_full\\n(\\n\")\n",
    "    for col in shodanJSON.columns[:-1]:\n",
    "        outfile.write(col + \" string,\\n\")\n",
    "    outfile.write(shodanJSON.columns[-1] + \" string\\n)\\nSTORED AS PARQUET\\n\" \\\n",
    "                  \"LOCATION '/user/svillanueva/spark_jsonRDD_pq_shodan_201501_FULL';\\n\\n\" \\\n",
    "                 \"--create empty pq full table\\n\" \\\n",
    "                  \"CREATE TABLE shodan.pq_shodan_201501_full\\n(\\n\")\n",
    "    for col in shodanJSON.columns[:-1]:\n",
    "        outfile.write(col + \" string,\\n\")\n",
    "    outfile.write(shodanJSON.columns[-1] + \" string\\n)\\nPARTITIONED BY (year_ts INT, month_ts INT, day_ts INT)\\n\" \\\n",
    "                  \"STORED AS PARQUET\\nTBLPROPERTIES ('parquet.compression'='SNAPPY');\\n\\n\" \\\n",
    "                 \"--insert data into pq_shodan_201501_full\\n\" \\\n",
    "                  \"INSERT INTO shodan.pq_shodan_201501_full\\n\" \\\n",
    "                 \"PARTITION(year_ts, month_ts, day_ts)\\nSELECT \")\n",
    "    for col in shodanJSON.columns:\n",
    "        outfile.write(col + \",\\n\")\n",
    "    outfile.write(\"YEAR(to_date(timestamp)),\\nMONTH(to_date(timestamp)),\\nDAY(to_date(timestamp))\\n\" \\\n",
    "                 \"FROM shodan.pq_shodan_staging_full\\n;\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
